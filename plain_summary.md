# Plain Language Summary

This study introduces a new model for how humans and animals perceive time, challenging traditional theories that rely on internal clocks or continuous processes. Instead, our model uses a hierarchical system based on comparing differences between two moments, like snapshots, to estimate elapsed time. By organizing these comparisons across multiple scales—like zooming in and out on a map—it captures how time feels longer or shorter depending on the intensity of changes in our environment. For example, a busy scene might feel longer than a calm one, even if both last the same number of seconds. The model predicts specific patterns in how people judge time, such as non-linear scaling and sensitivity to brain chemistry changes, which we plan to test in experiments with human participants and computer simulations. These experiments involve showing people moving dot patterns and asking them to compare time intervals, as well as testing robotic agents in virtual tasks. Our framework offers a fresh perspective on time perception, suggesting it’s less about counting seconds and more about measuring change. All materials, including code and figures, are publicly available. This work is purely theoretical and computational, involving no human or animal subjects. (Word count: 184)
